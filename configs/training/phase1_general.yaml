# Phase 1: General Chess Understanding
# Train on 95% regular games + 5% checkmate games

phase: 1

# Training params
num_epochs: 8
batch_size: 4  # Small for M1 Mac (20GB MPS limit)
gradient_accumulation_steps: 32  # Effective batch size: 128

# Learning rate
learning_rate: 3e-4
warmup_steps: 1000
lr_scheduler: cosine

# Regularization
weight_decay: 0.01
gradient_clip_norm: 1.0

# Mixed precision
mixed_precision: bf16
gradient_checkpointing: false  # TODO: ChessTransformer doesn't support this yet

# Checkpointing
save_steps: 0  # Save per epoch
save_total_limit: 3
load_best_model_at_end: true

# Evaluation
eval_steps: 0  # Evaluate per epoch
metric_for_best_model: eval_loss

# Logging
logging_steps: 10
# report_to: None  # Disable all integrations (can be overridden by WANDB_MODE env var)

# Data mixing
checkmate_ratio: 0.05    # 5% checkmate examples
checkmate_weight: 1.0    # Equal weighting
