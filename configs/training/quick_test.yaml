# Quick test training configuration
# For rapid iteration and testing with small data sample

phase: 1
num_epochs: 2  # Very short for quick test
batch_size: 32
gradient_accumulation_steps: 1
learning_rate: 3e-4
warmup_steps: 50
lr_scheduler: cosine
weight_decay: 0.01
gradient_clip_norm: 1.0

# Mixed precision
mixed_precision: bf16  # Use fp16 if bf16 not available
gradient_checkpointing: false  # Disabled for speed in quick test

# Logging and evaluation
logging_steps: 10
eval_steps: 50
save_steps: 100
save_total_limit: 2

# Early stopping
load_best_model_at_end: true
metric_for_best_model: eval_loss

# WandB logging (optional)
report_to: []  # Empty = no logging for quick test

# Checkmate training (Phase 1)
checkmate_ratio: 0.0  # No checkmate data for quick test
checkmate_weight: 1.0
