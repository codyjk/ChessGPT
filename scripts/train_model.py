import argparse

import torch
from torch.utils.data import DataLoader

from chess_model import (
    ChessDataset,
    ChessTokenizer,
    ChessTransformer,
    calculate_random_baseline,
    get_device,
    train_model,
)

DEFAULT_MAX_LENGTH = 50
DEFAULT_NUM_EMBEDDINGS = 256
DEFAULT_NUM_EPOCHS = 10
DEFAULT_MODEL_OUTPUT_FILE = "out/chess_transformer_model.pth"
DEFAULT_TOKENIZER_FILE = "out/chess_tokenizer.json"
DEFAULT_INITIAL_LEARNING_RATE = 1e-3
DEFAULT_BATCH_SIZE = 128
DEFAULT_NUM_LAYERS = 4
DEFAULT_NUM_HEADS = 4
DEFAULT_SHOW_RANDOM_BASELINE = True


def main():
    """
    Usage: poetry run train-model --inpu-training-data-file out/training-data.csv --input-validation-data-file out/validation-data.csv
    """

    parser = build_arg_parser()
    args = parser.parse_args()
    print_training_header(args)

    # Initialize tokenizer and model
    print("Loading tokenizer...")
    tokenizer = ChessTokenizer.load(args.input_tokenizer_file)
    print(f"Tokenizer initialized with vocab_size={tokenizer.vocab_size}")

    model = ChessTransformer(
        vocab_size=tokenizer.vocab_size,
        n_positions=args.max_context_length,
        n_embd=args.num_embeddings,
        n_layer=args.num_layers,
        n_head=args.num_heads,
    )

    if args.input_state_dict_file:
        print(f"Initializing model from {args.input_state_dict_file}...")
        model.load_state_dict(torch.load(args.input_state_dict_file))

    # Load and prepare data
    print("Loading training/validation data...")
    train_dataset = ChessDataset(
        args.input_training_data_file, tokenizer, max_length=args.max_context_length
    )
    val_dataset = ChessDataset(
        args.input_validation_data_file, tokenizer, max_length=args.max_context_length
    )

    train_dataloader = DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True
    )
    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size)

    # Get the appropriate device
    device = get_device()
    print(f"Using device: {device}")

    # Calculate random baseline loss (if requested)
    if args.show_random_baseline:
        random_baseline_loss = calculate_random_baseline(
            train_dataloader, model.config.vocab_size, device
        )
        print(f"Random baseline loss: {random_baseline_loss:.4f}")

    # Train the model
    trained_model = train_model(
        model,
        train_dataloader,
        val_dataloader,
        num_epochs=args.num_epochs,
        learning_rate=1e-3,
        device=device,
    )

    # Save the trained model
    torch.save(trained_model.state_dict(), args.output_model_file)
    print(f"Model saved to: {args.output_model_file}")


def build_arg_parser():
    parser = argparse.ArgumentParser(description="Train the LLM.")

    # Input files
    parser.add_argument(
        "--input-training-data-file",
        type=str,
        help="The input training data file, as returned by `poetry run prepare-training-data`",
        required=True,
    )
    parser.add_argument(
        "--input-validation-data-file",
        type=str,
        help="The input validation data file, as returned by `poetry run prepare-training-data`",
        required=True,
    )
    parser.add_argument(
        "--input-tokenizer-file",
        type=str,
        help=f"The tokenizer file, as generated by `poetry run fit-and-save-tokenizer`. Defaults to {DEFAULT_TOKENIZER_FILE}",
        required=False,
        default=DEFAULT_TOKENIZER_FILE,
    )
    parser.add_argument(
        "--input-state-dict-file",
        type=str,
        help="The state dict file to load the initial model from. If not provided, the model will be randomly initialized.",
        required=False,
        default=None,
    )
    parser.add_argument(
        "--output-model-file",
        type=str,
        help=f"Where to save the `.pth` file for the trained model. Default: {DEFAULT_MODEL_OUTPUT_FILE}",
        required=False,
        default=DEFAULT_MODEL_OUTPUT_FILE,
    )

    # Hyperparameters
    parser.add_argument(
        "--max-context-length",
        type=int,
        help=f"The maximum context length (number of moves) to train against. Default: {DEFAULT_MAX_LENGTH}",
        required=False,
        default=DEFAULT_MAX_LENGTH,
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        help=f"The batch size to use. Default: {DEFAULT_BATCH_SIZE}",
        required=False,
        default=DEFAULT_BATCH_SIZE,
    )
    parser.add_argument(
        "--num-embeddings",
        type=int,
        help=f"The number of embeddings to use in the model. Default: {DEFAULT_NUM_EMBEDDINGS}",
        required=False,
        default=DEFAULT_NUM_EMBEDDINGS,
    )
    parser.add_argument(
        "--num-layers",
        type=int,
        help=f"The number of layers to use in the model. Default: {DEFAULT_NUM_LAYERS}",
        required=False,
        default=DEFAULT_NUM_LAYERS,
    )
    parser.add_argument(
        "--num-heads",
        type=int,
        help=f"The number of heads to use in the model. Default: {DEFAULT_NUM_HEADS}",
        required=False,
        default=DEFAULT_NUM_HEADS,
    )
    parser.add_argument(
        "--num-epochs",
        type=int,
        help=f"The number of epochs to train the model for. Default: {DEFAULT_NUM_EPOCHS}",
        required=False,
        default=DEFAULT_NUM_EPOCHS,
    )
    parser.add_argument(
        "--initial-learning-rate",
        type=float,
        help=f"The initial learning rate to use. Default: {DEFAULT_INITIAL_LEARNING_RATE}",
        required=False,
        default=DEFAULT_INITIAL_LEARNING_RATE,
    )
    parser.add_argument(
        "--show-random-baseline",
        type=bool,
        help=f"Whether to show the random baseline loss. Default: {DEFAULT_SHOW_RANDOM_BASELINE}",
        required=False,
        default=DEFAULT_SHOW_RANDOM_BASELINE,
    )

    return parser


def print_training_header(args):
    print(
        "###################################################################################################"
    )
    print("## Training model with args:")
    print(f"Training data:          {args.input_training_data_file}")
    print(f"Validation data:        {args.input_validation_data_file}")
    print(f"Tokenizer file:         {args.input_tokenizer_file}")
    print(f"State dict file:        {args.input_state_dict_file}")
    print(f"Model output file:      {args.output_model_file}")
    print(f"Max length:             {args.max_context_length}")
    print(f"Batch size:             {args.batch_size}")
    print(f"Num embeddings:         {args.num_embeddings}")
    print(f"Num layers:             {args.num_layers}")
    print(f"Num heads:              {args.num_heads}")
    print(f"Num training epochs:    {args.num_epochs}")
    print(f"Initial learning rate:  {args.initial_learning_rate}")
    print(
        "###################################################################################################"
    )
