{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f1c99-2b7d-4f45-945c-6ec7607da5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chess_model import fit_tokenizer, ChessTransformer\n",
    "\n",
    "# Change these to whatever the model was trained with!\n",
    "# Default length is 10, embeddings is 128.\n",
    "MAX_LEN = 50\n",
    "N_EMBD = 64\n",
    "TRAINING_DATA_FILE = \"out/training-data-trunc.csv\"\n",
    "MODEL_OUTPUT_FILE = \"out/chess_transformer_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b430031-c183-4e0a-b9cb-e66d35e00e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = fit_tokenizer(f\"../../{TRAINING_DATA_FILE}\")\n",
    "print(f'Tokenizer initialized with vocab_size={tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a4ffb-225f-46fb-af76-f7bf81e78db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChessTransformer(vocab_size=tokenizer.vocab_size, n_positions=MAX_LEN, n_embd=N_EMBD) # use defaults for small model\n",
    "model.load_state_dict(torch.load(f\"../../{MODEL_OUTPUT_FILE}\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380f610-8082-4b57-8c4b-88075248dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def preprocess_input(move_sequence, tokenizer, max_length=MAX_LEN):\n",
    "    # Tokenize the input sequence\n",
    "    input_ids = tokenizer.encode(move_sequence)\n",
    "    \n",
    "    # Truncate or pad the sequence to max_length\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[-max_length:]\n",
    "    else:\n",
    "        input_ids = [0] * (max_length - len(input_ids)) + input_ids\n",
    "\n",
    "    print(f\"with padding {input_ids}\")\n",
    "    \n",
    "    return torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def predict_next_move(model, tokenizer, move_sequence, device, temperature=1.0, top_k=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Preprocess the input\n",
    "    input_ids = preprocess_input(move_sequence, tokenizer).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        move_logits = model(input_ids)\n",
    "    \n",
    "    # Apply temperature to logits\n",
    "    move_logits = move_logits / temperature\n",
    "    \n",
    "    # Get probabilities\n",
    "    move_probs = F.softmax(move_logits, dim=-1)\n",
    "    \n",
    "    # Normalize probabilities after zeroing out the last move\n",
    "    move_probs = move_probs / move_probs.sum()\n",
    "    \n",
    "    # Get top-k moves\n",
    "    top_k_probs, top_k_indices = torch.topk(move_probs, top_k)\n",
    "    \n",
    "    # Sample from top-k moves\n",
    "    sampled_index = torch.multinomial(top_k_probs.squeeze(), 1).item()\n",
    "    predicted_move_id = top_k_indices.squeeze()[sampled_index].item()\n",
    "    predicted_move = tokenizer.decode([predicted_move_id])\n",
    "\n",
    "    return predicted_move, move_probs\n",
    "\n",
    "def interpret_prediction(predicted_move, move_probs, tokenizer):\n",
    "    print(f\"Predicted next move: {predicted_move}\")\n",
    "    \n",
    "    # Debugging information\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer.move_to_id)}\")\n",
    "    print(\"Top 5 predicted moves:\")\n",
    "    top_moves = torch.topk(move_probs.squeeze(), 5)\n",
    "    for i, (prob, idx) in enumerate(zip(top_moves.values, top_moves.indices)):\n",
    "        move = tokenizer.decode([idx.item()])\n",
    "        print(f\"  {i+1}. {move} (probability: {prob.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79903883-763e-4731-adbe-3d675aaf5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example move sequence\n",
    "move_sequence = [\"e4\"]\n",
    "\n",
    "# Make multiple predictions\n",
    "for _ in range(5):\n",
    "    predicted_move, move_probs = predict_next_move(model, tokenizer, move_sequence, device, temperature=0.8, top_k=5)\n",
    "    print(\"\\n--- New Prediction ---\")\n",
    "    interpret_prediction(predicted_move, move_probs, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
