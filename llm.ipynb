{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed846484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from typing import Iterator, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_game(game: str) -> Iterator[Tuple[str, str, str, str]]:\n",
    "    \"\"\"\n",
    "    Process a single game and yield (context, next_move, is_checkmate, outcome) tuples.\n",
    "    \"\"\"\n",
    "    moves = game.split()\n",
    "    outcome = moves[-1]\n",
    "    moves = moves[:-1]  # Remove the outcome from the move list\n",
    "    \n",
    "    for i in range(len(moves)):\n",
    "        context = \" \".join(moves[:i])\n",
    "        next_move = moves[i]\n",
    "        is_checkmate = \"1\" if next_move.endswith(\"#\") else \"0\"\n",
    "        \n",
    "        # For the last move, we know the outcome\n",
    "        if i == len(moves) - 1:\n",
    "            yield context, next_move, is_checkmate, outcome\n",
    "        else:\n",
    "            yield context, next_move, is_checkmate, \"\"\n",
    "\n",
    "def preprocess_data(input_file: str, train_file: str, val_file: str, max_context_length: int = 50, val_split: float = 0.1):\n",
    "    \"\"\"\n",
    "    Preprocess the input file and write (context, next_move, is_checkmate, outcome) tuples to train and val files.\n",
    "    \"\"\"\n",
    "    # Open both output files\n",
    "    with open(train_file, 'w', newline='') as train_outfile, open(val_file, 'w', newline='') as val_outfile:\n",
    "        train_writer = csv.writer(train_outfile)\n",
    "        val_writer = csv.writer(val_outfile)\n",
    "        \n",
    "        # Write headers\n",
    "        headers = ['context', 'next_move', 'is_checkmate', 'outcome']\n",
    "        train_writer.writerow(headers)\n",
    "        val_writer.writerow(headers)\n",
    "        \n",
    "        # Count total lines for progress bar\n",
    "        total_lines = sum(1 for _ in open(input_file, 'r'))\n",
    "        \n",
    "        # Process the input file line by line\n",
    "        with open(input_file, 'r') as infile:\n",
    "            for line in tqdm(infile, total=total_lines, desc=\"Processing games\"):\n",
    "                game = line.strip()\n",
    "                for context, next_move, is_checkmate, outcome in process_game(game):\n",
    "                    # Limit context to last `max_context_length` moves\n",
    "                    context_moves = context.split()[-max_context_length:]\n",
    "                    limited_context = \" \".join(context_moves)\n",
    "                    \n",
    "                    # Decide whether to write to train or val file\n",
    "                    if random.random() < val_split:\n",
    "                        val_writer.writerow([limited_context, next_move, is_checkmate, outcome])\n",
    "                    else:\n",
    "                        train_writer.writerow([limited_context, next_move, is_checkmate, outcome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(\"out/grandmaster-truncated.txt\", \"out/training-data.csv\", \"out/validation-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "class ChessTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_positions=50, n_embd=128, n_layer=2, n_head=2):\n",
    "        super(ChessTransformer, self).__init__()\n",
    "        \n",
    "        self.config = GPT2Config(\n",
    "            vocab_size=vocab_size,\n",
    "            n_positions=n_positions,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head\n",
    "        )\n",
    "        \n",
    "        self.transformer = GPT2Model(self.config)\n",
    "        self.move_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.checkmate_head = nn.Linear(n_embd, 1)\n",
    "        self.outcome_head = nn.Linear(n_embd, 3)  # Win, Loss, Draw\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Use the second to last token's hidden state for prediction\n",
    "        # This way, we're predicting based on the context, not just repeating the last move\n",
    "        prediction_hidden_state = hidden_states[:, -2, :]\n",
    "        \n",
    "        # Predict next move\n",
    "        move_logits = self.move_head(prediction_hidden_state)\n",
    "        \n",
    "        # Predict checkmate probability\n",
    "        checkmate_logits = self.checkmate_head(prediction_hidden_state)\n",
    "        \n",
    "        # Predict game outcome\n",
    "        outcome_logits = self.outcome_head(prediction_hidden_state)\n",
    "        \n",
    "        return move_logits, checkmate_logits, outcome_logits\n",
    "        \n",
    "class ChessTokenizer:\n",
    "    def __init__(self):\n",
    "        # Create a vocabulary of chess moves\n",
    "        # This is a simplified version and should be expanded based on your data\n",
    "        self.move_to_id = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "        self.id_to_move = {0: \"[PAD]\", 1: \"[UNK]\"}\n",
    "        self.vocab_size = 2  # Start with PAD and UNK tokens\n",
    "\n",
    "    def fit(self, moves):\n",
    "        for move in moves:\n",
    "            if move not in self.move_to_id:\n",
    "                self.move_to_id[move] = self.vocab_size\n",
    "                self.id_to_move[self.vocab_size] = move\n",
    "                self.vocab_size += 1\n",
    "\n",
    "    def encode(self, moves):\n",
    "        return [self.move_to_id.get(move, self.move_to_id[\"[UNK]\"]) for move in moves]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.id_to_move.get(id, \"[UNK]\") for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=50):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        context = row['context']\n",
    "        if pd.isna(context) or context == '':\n",
    "            context = []\n",
    "        else:\n",
    "            context = str(context).split()\n",
    "        next_move = row['next_move']\n",
    "        is_checkmate = float(row['is_checkmate'])\n",
    "        outcome = row['outcome']\n",
    "\n",
    "        # Tokenize input (context + next_move)\n",
    "        input_ids = self.tokenizer.encode(context + [next_move])\n",
    "        input_ids = input_ids[-self.max_length:]  # Keep only the last max_length tokens\n",
    "        input_ids = [0] * (self.max_length - len(input_ids)) + input_ids  # Pad from the left\n",
    "\n",
    "        # Create labels (next_move)\n",
    "        labels = self.tokenizer.encode([next_move])[0]\n",
    "        \n",
    "        # Convert outcome to one-hot encoding\n",
    "        outcome_label = torch.zeros(3, dtype=torch.float)\n",
    "        if outcome == '1-0':\n",
    "            outcome_label[0] = 1\n",
    "        elif outcome == '0-1':\n",
    "            outcome_label[1] = 1\n",
    "        elif outcome == '1/2-1/2':\n",
    "            outcome_label[2] = 1\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'is_checkmate': torch.tensor(is_checkmate, dtype=torch.float),\n",
    "            'outcome': outcome_label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def fit_tokenizer(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    all_moves = []\n",
    "    for _, row in data.iterrows():\n",
    "        context = str(row['context']).split() if not pd.isna(row['context']) else []\n",
    "        all_moves.extend(context)\n",
    "        all_moves.append(row['next_move'])\n",
    "    \n",
    "    tokenizer = ChessTokenizer()\n",
    "    tokenizer.fit(all_moves)\n",
    "    return tokenizer\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=50):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        context = str(row['context']).split() if not pd.isna(row['context']) else []\n",
    "        next_move = row['next_move']\n",
    "        is_checkmate = float(row['is_checkmate'])\n",
    "        outcome = row['outcome']\n",
    "\n",
    "        # Tokenize input (context + next_move)\n",
    "        input_ids = self.tokenizer.encode(context + [next_move])\n",
    "        input_ids = input_ids[-self.max_length:]  # Keep only the last max_length tokens\n",
    "        input_ids = [0] * (self.max_length - len(input_ids)) + input_ids  # Pad from the left\n",
    "\n",
    "        # Create labels (next_move)\n",
    "        labels = self.tokenizer.encode([next_move])[0]\n",
    "        \n",
    "        # Convert outcome to one-hot encoding (as float)\n",
    "        outcome_label = torch.zeros(3, dtype=torch.float)\n",
    "        if outcome == '1-0':\n",
    "            outcome_label[0] = 1.0\n",
    "        elif outcome == '0-1':\n",
    "            outcome_label[1] = 1.0\n",
    "        elif outcome == '1/2-1/2':\n",
    "            outcome_label[2] = 1.0\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'is_checkmate': torch.tensor(is_checkmate, dtype=torch.float),\n",
    "            'outcome': outcome_label\n",
    "        }\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    move_criterion = nn.CrossEntropyLoss()\n",
    "    checkmate_criterion = nn.BCEWithLogitsLoss()\n",
    "    outcome_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    total_steps = num_epochs * len(train_dataloader)\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Training Progress\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            move_labels = batch['labels'].to(device)\n",
    "            checkmate_labels = batch['is_checkmate'].to(device)\n",
    "            outcome_labels = batch['outcome'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            move_logits, checkmate_logits, outcome_logits = model(input_ids)\n",
    "\n",
    "            # Calculate losses\n",
    "            move_loss = move_criterion(move_logits, move_labels)\n",
    "            checkmate_loss = checkmate_criterion(checkmate_logits.squeeze(), checkmate_labels)\n",
    "            outcome_loss = outcome_criterion(outcome_logits, outcome_labels)\n",
    "\n",
    "            # Combine losses (you can adjust the weights)\n",
    "            loss = move_loss + 0.1 * checkmate_loss + 0.1 * outcome_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({'epoch': epoch+1, 'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                move_labels = batch['labels'].to(device)\n",
    "                checkmate_labels = batch['is_checkmate'].to(device)\n",
    "                outcome_labels = batch['outcome'].to(device)\n",
    "\n",
    "                move_logits, checkmate_logits, outcome_logits = model(input_ids)\n",
    "\n",
    "                move_loss = move_criterion(move_logits, move_labels)\n",
    "                checkmate_loss = checkmate_criterion(checkmate_logits.squeeze(), checkmate_labels)\n",
    "                outcome_loss = outcome_criterion(outcome_logits, outcome_labels)\n",
    "\n",
    "                loss = move_loss + 0.1 * checkmate_loss + 0.1 * outcome_loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_random_baseline(dataloader, vocab_size, device):\n",
    "    total_loss = 0\n",
    "    move_criterion = nn.CrossEntropyLoss()\n",
    "    checkmate_criterion = nn.BCEWithLogitsLoss()\n",
    "    outcome_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Calculating random baseline\"):\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        \n",
    "        # Generate random logits\n",
    "        random_move_logits = torch.rand(batch_size, vocab_size).to(device)\n",
    "        random_checkmate_logits = torch.rand(batch_size, 1).to(device)\n",
    "        random_outcome_logits = torch.rand(batch_size, 3).to(device)\n",
    "\n",
    "        # Calculate losses\n",
    "        move_loss = move_criterion(random_move_logits, batch['labels'].to(device))\n",
    "        checkmate_loss = checkmate_criterion(random_checkmate_logits.squeeze(), batch['is_checkmate'].to(device))\n",
    "        outcome_loss = outcome_criterion(random_outcome_logits, batch['outcome'].to(device))\n",
    "\n",
    "        loss = move_loss + 0.1 * checkmate_loss + 0.1 * outcome_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8da186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "print(f'Using device {get_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5337f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = fit_tokenizer('out/training-data.csv')\n",
    "model = ChessTransformer(vocab_size=tokenizer.vocab_size, n_positions=25, n_embd=128, n_layer=2, n_head=2)\n",
    "\n",
    "# Load and prepare data\n",
    "train_dataset = ChessDataset('out/training-data.csv', tokenizer, max_length=25)\n",
    "val_dataset = ChessDataset('out/validation-data.csv', tokenizer, max_length=25)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Get the appropriate device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Calculate random baseline loss\n",
    "random_baseline_loss = calculate_random_baseline(train_dataloader, model.config.vocab_size, device)\n",
    "print(f\"Random Baseline Loss: {random_baseline_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_dataloader, val_dataloader, num_epochs=2, learning_rate=1e-3, device=device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'chess_transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac76c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def preprocess_input(move_sequence, tokenizer, max_length=25):\n",
    "    \"\"\"\n",
    "    Preprocess the input move sequence for the model.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence\n",
    "    input_ids = tokenizer.encode(move_sequence)\n",
    "    \n",
    "    # Truncate or pad the sequence to max_length\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[-max_length:]\n",
    "    else:\n",
    "        input_ids = [0] * (max_length - len(input_ids)) + input_ids\n",
    "    \n",
    "    return torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def predict_next_move(model, tokenizer, move_sequence, device, temperature=1.0, top_k=5):\n",
    "    \"\"\"\n",
    "    Predict the next move, checkmate probability, and game outcome.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Preprocess the input\n",
    "    input_ids = preprocess_input(move_sequence, tokenizer).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        move_logits, checkmate_logits, outcome_logits = model(input_ids)\n",
    "    \n",
    "    # Apply temperature to logits\n",
    "    move_logits = move_logits / temperature\n",
    "    \n",
    "    # Get probabilities\n",
    "    move_probs = F.softmax(move_logits, dim=-1)\n",
    "    \n",
    "    # Zero out the probability of the last move in the sequence\n",
    "    last_move_id = tokenizer.encode([move_sequence[-1]])[0]\n",
    "    move_probs[0, last_move_id] = 0\n",
    "    \n",
    "    # Normalize probabilities after zeroing out the last move\n",
    "    move_probs = move_probs / move_probs.sum()\n",
    "    \n",
    "    # Get top-k moves\n",
    "    top_k_probs, top_k_indices = torch.topk(move_probs, top_k)\n",
    "    \n",
    "    # Sample from top-k moves\n",
    "    sampled_index = torch.multinomial(top_k_probs.squeeze(), 1).item()\n",
    "    predicted_move_id = top_k_indices.squeeze()[sampled_index].item()\n",
    "    predicted_move = tokenizer.decode([predicted_move_id])\n",
    "    \n",
    "    # Get the checkmate probability\n",
    "    checkmate_prob = torch.sigmoid(checkmate_logits).item()\n",
    "    \n",
    "    # Get the game outcome probabilities\n",
    "    outcome_probs = F.softmax(outcome_logits, dim=-1).squeeze()\n",
    "    \n",
    "    return predicted_move, checkmate_prob, outcome_probs, move_probs\n",
    "\n",
    "def interpret_prediction(predicted_move, checkmate_prob, outcome_probs, move_probs, tokenizer):\n",
    "    \"\"\"\n",
    "    Interpret the model's prediction in a human-readable format.\n",
    "    \"\"\"\n",
    "    outcomes = ['Win', 'Loss', 'Draw']\n",
    "    outcome_dict = {outcome: prob.item() for outcome, prob in zip(outcomes, outcome_probs)}\n",
    "    most_likely_outcome = max(outcome_dict, key=outcome_dict.get)\n",
    "    \n",
    "    print(f\"Predicted next move: {predicted_move}\")\n",
    "    print(f\"Checkmate probability: {checkmate_prob:.2f}\")\n",
    "    print(\"Game outcome probabilities:\")\n",
    "    for outcome, prob in outcome_dict.items():\n",
    "        print(f\"  {outcome}: {prob:.2f}\")\n",
    "    print(f\"Most likely outcome: {most_likely_outcome}\")\n",
    "    \n",
    "    # Debugging information\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer.move_to_id)}\")\n",
    "    print(\"Top 5 predicted moves:\")\n",
    "    top_moves = torch.topk(move_probs.squeeze(), 5)\n",
    "    for i, (prob, idx) in enumerate(zip(top_moves.values, top_moves.indices)):\n",
    "        move = tokenizer.decode([idx.item()])\n",
    "        print(f\"  {i+1}. {move} (probability: {prob.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model and tokenizer\n",
    "# tokenizer = torch.load('chess_tokenizer.pth') # tokenizer is already in memory\n",
    "model = ChessTransformer(vocab_size=len(tokenizer.move_to_id), n_positions=25, n_embd=128, n_layer=2, n_head=2)\n",
    "model.load_state_dict(torch.load('chess_transformer_model.pth'))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aaa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example move sequence\n",
    "move_sequence = [\"e4\", \"e5\", \"Nf3\", \"Nc6\", \"Bb5\"]\n",
    "\n",
    "# Make multiple predictions\n",
    "for _ in range(5):\n",
    "    predicted_move, checkmate_prob, outcome_probs, move_probs = predict_next_move(model, tokenizer, move_sequence, device, temperature=0.8, top_k=5)\n",
    "    print(\"\\n--- New Prediction ---\")\n",
    "    interpret_prediction(predicted_move, checkmate_prob, outcome_probs, move_probs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815935d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
