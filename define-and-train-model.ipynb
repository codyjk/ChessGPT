{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abfb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc66636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60790649"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_game(game_str):\n",
    "    moves = game_str.split()\n",
    "    input_output_pairs = []\n",
    "    for i in range(1, len(moves)):\n",
    "        input_seq = ' '.join(moves[:i])\n",
    "        output_move = moves[i]\n",
    "        input_output_pairs.append((input_seq, output_move))\n",
    "    return input_output_pairs\n",
    "\n",
    "def preprocess_file(file_path):\n",
    "    all_pairs = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            game = line.strip()\n",
    "            all_pairs.extend(preprocess_game(game))\n",
    "    return all_pairs\n",
    "\n",
    "file_path = 'out/grandmaster.txt'\n",
    "training_data = preprocess_file(file_path)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3239c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "files = [\n",
    "    'out/beginner.txt',\n",
    "    'out/intermediate.txt',\n",
    "    'out/master.txt',\n",
    "    'out/grandmaster.txt',\n",
    "]\n",
    "\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            for word in line.split(' '):\n",
    "                vocab.add(word.strip())\n",
    "vocab = sorted(list(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b87167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as file:\n",
    "    file.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c064db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-1',\n",
       " '1-0',\n",
       " '1/2-1/2',\n",
       " 'B1a3',\n",
       " 'B1a4',\n",
       " 'B1b2',\n",
       " 'B1b3',\n",
       " 'B1c2',\n",
       " 'B1c2+',\n",
       " 'B1c3',\n",
       " 'B1d2',\n",
       " 'B1d3',\n",
       " 'B1d4',\n",
       " 'B1e2',\n",
       " 'B1e3',\n",
       " 'B1e4',\n",
       " 'B1f2',\n",
       " 'B1f3',\n",
       " 'B1f3+',\n",
       " 'B1g2',\n",
       " 'B1g3',\n",
       " 'B1h3',\n",
       " 'B1xg2',\n",
       " 'B2a3',\n",
       " 'B2b3',\n",
       " 'B2c3',\n",
       " 'B2c4',\n",
       " 'B2d3',\n",
       " 'B2e3',\n",
       " 'B2f3',\n",
       " 'B2f5',\n",
       " 'B2g3',\n",
       " 'B2h3',\n",
       " 'B2xf3',\n",
       " 'B2xf3#',\n",
       " 'B3b2',\n",
       " 'B3b4',\n",
       " 'B3b5',\n",
       " 'B3c2',\n",
       " 'B3c4',\n",
       " 'B3c5',\n",
       " 'B3d2',\n",
       " 'B3d4',\n",
       " 'B3e2',\n",
       " 'B3e4',\n",
       " 'B3f2',\n",
       " 'B3f4',\n",
       " 'B3g2',\n",
       " 'B3g4',\n",
       " 'B3h2',\n",
       " 'B3h5',\n",
       " 'B4b3',\n",
       " 'B4c5',\n",
       " 'B4d3',\n",
       " 'B4d5',\n",
       " 'B4e3',\n",
       " 'B4e5',\n",
       " 'B4e6',\n",
       " 'B4f3',\n",
       " 'B4f5',\n",
       " 'B4f6',\n",
       " 'B4g3',\n",
       " 'B4g5',\n",
       " 'B4xf5',\n",
       " 'B5a4',\n",
       " 'B5b4',\n",
       " 'B5b6',\n",
       " 'B5c3',\n",
       " 'B5c4',\n",
       " 'B5c6',\n",
       " 'B5d4',\n",
       " 'B5d6',\n",
       " 'B5e4',\n",
       " 'B5e6',\n",
       " 'B5f4',\n",
       " 'B5f6',\n",
       " 'B5h4',\n",
       " 'B5h6',\n",
       " 'B6a4',\n",
       " 'B6a5',\n",
       " 'B6b5',\n",
       " 'B6b7',\n",
       " 'B6c5',\n",
       " 'B6c7',\n",
       " 'B6d4',\n",
       " 'B6d5',\n",
       " 'B6d7',\n",
       " 'B6e5',\n",
       " 'B6e7',\n",
       " 'B6f4',\n",
       " 'B6f5',\n",
       " 'B6f5+',\n",
       " 'B6f7',\n",
       " 'B6g5',\n",
       " 'B6g7',\n",
       " 'B6h5',\n",
       " 'B7a6',\n",
       " 'B7b6',\n",
       " 'B7c5+',\n",
       " 'B7c6',\n",
       " 'B7d4',\n",
       " 'B7d5',\n",
       " 'B7d6',\n",
       " 'B7e6',\n",
       " 'B7f5',\n",
       " 'B7f6',\n",
       " 'B7g5',\n",
       " 'B7g6',\n",
       " 'B7xf5',\n",
       " 'B7xg6',\n",
       " 'B8a5',\n",
       " 'B8a7',\n",
       " 'B8b6',\n",
       " 'B8c7',\n",
       " 'B8d6',\n",
       " 'B8d7',\n",
       " 'B8e5',\n",
       " 'B8e6',\n",
       " 'B8e7',\n",
       " 'B8f7',\n",
       " 'B8g6',\n",
       " 'B8g7',\n",
       " 'B8xb5',\n",
       " 'B8xg5+',\n",
       " 'B8xg6',\n",
       " 'B8xg6#',\n",
       " 'Ba1',\n",
       " 'Ba1#',\n",
       " 'Ba1+',\n",
       " 'Ba2',\n",
       " 'Ba2#',\n",
       " 'Ba2+',\n",
       " 'Ba3',\n",
       " 'Ba3#',\n",
       " 'Ba3+',\n",
       " 'Ba4',\n",
       " 'Ba4#',\n",
       " 'Ba4+',\n",
       " 'Ba5',\n",
       " 'Ba5#',\n",
       " 'Ba5+',\n",
       " 'Ba6',\n",
       " 'Ba6#',\n",
       " 'Ba6+',\n",
       " 'Ba7',\n",
       " 'Ba7#',\n",
       " 'Ba7+',\n",
       " 'Ba8',\n",
       " 'Ba8#',\n",
       " 'Ba8+',\n",
       " 'Bab1',\n",
       " 'Bab2',\n",
       " 'Bab2+',\n",
       " 'Bab3',\n",
       " 'Bab3+',\n",
       " 'Bab4',\n",
       " 'Bab5',\n",
       " 'Bab5+',\n",
       " 'Bab6',\n",
       " 'Bab6+',\n",
       " 'Bab7',\n",
       " 'Bab8',\n",
       " 'Bac1',\n",
       " 'Bac2',\n",
       " 'Bac3',\n",
       " 'Bac3+',\n",
       " 'Bac4',\n",
       " 'Bac4+',\n",
       " 'Bac5',\n",
       " 'Bac5+',\n",
       " 'Bac6',\n",
       " 'Bac6#',\n",
       " 'Bac6+',\n",
       " 'Bac7',\n",
       " 'Bad2',\n",
       " 'Bad3',\n",
       " 'Bad3+',\n",
       " 'Bad4',\n",
       " 'Bad4+',\n",
       " 'Bad5',\n",
       " 'Bad5#',\n",
       " 'Bad5+',\n",
       " 'Bad6',\n",
       " 'Bad7',\n",
       " 'Bad7#',\n",
       " 'Bae1',\n",
       " 'Bae2',\n",
       " 'Bae3',\n",
       " 'Bae4',\n",
       " 'Bae4+',\n",
       " 'Bae5',\n",
       " 'Bae5+',\n",
       " 'Bae6',\n",
       " 'Bae7',\n",
       " 'Bae8',\n",
       " 'Baf3',\n",
       " 'Baf6',\n",
       " 'Baf6+',\n",
       " 'Baf7',\n",
       " 'Baf8',\n",
       " 'Bag2',\n",
       " 'Bag7',\n",
       " 'Baxb3',\n",
       " 'Baxc7',\n",
       " 'Baxe5',\n",
       " 'Baxf3',\n",
       " 'Baxf6+',\n",
       " 'Baxg2',\n",
       " 'Baxg7',\n",
       " 'Bb1',\n",
       " 'Bb1#',\n",
       " 'Bb1+',\n",
       " 'Bb2',\n",
       " 'Bb2#',\n",
       " 'Bb2+',\n",
       " 'Bb3',\n",
       " 'Bb3#',\n",
       " 'Bb3+',\n",
       " 'Bb4',\n",
       " 'Bb4#',\n",
       " 'Bb4+',\n",
       " 'Bb5',\n",
       " 'Bb5#',\n",
       " 'Bb5+',\n",
       " 'Bb6',\n",
       " 'Bb6#',\n",
       " 'Bb6+',\n",
       " 'Bb7',\n",
       " 'Bb7#',\n",
       " 'Bb7+',\n",
       " 'Bb8',\n",
       " 'Bb8#',\n",
       " 'Bb8+',\n",
       " 'Bb8d6',\n",
       " 'Bba2',\n",
       " 'Bba3',\n",
       " 'Bba4',\n",
       " 'Bba6',\n",
       " 'Bba7',\n",
       " 'Bbc1',\n",
       " 'Bbc2',\n",
       " 'Bbc2+',\n",
       " 'Bbc3',\n",
       " 'Bbc3+',\n",
       " 'Bbc4',\n",
       " 'Bbc4+',\n",
       " 'Bbc5',\n",
       " 'Bbc5+',\n",
       " 'Bbc6',\n",
       " 'Bbc6+',\n",
       " 'Bbc7',\n",
       " 'Bbc7+',\n",
       " 'Bbc8',\n",
       " 'Bbd2',\n",
       " 'Bbd3',\n",
       " 'Bbd3+',\n",
       " 'Bbd4',\n",
       " 'Bbd4#',\n",
       " 'Bbd4+',\n",
       " 'Bbd5',\n",
       " 'Bbd5+',\n",
       " 'Bbd6',\n",
       " 'Bbd6#',\n",
       " 'Bbd6+',\n",
       " 'Bbd7',\n",
       " 'Bbd8',\n",
       " 'Bbe2',\n",
       " 'Bbe3',\n",
       " 'Bbe4',\n",
       " 'Bbe4#',\n",
       " 'Bbe4+',\n",
       " 'Bbe5',\n",
       " 'Bbe5+',\n",
       " 'Bbe6+',\n",
       " 'Bbe7',\n",
       " 'Bbe8',\n",
       " 'Bbf2',\n",
       " 'Bbf2+',\n",
       " 'Bbf3+',\n",
       " 'Bbf4',\n",
       " 'Bbf4+',\n",
       " 'Bbf5',\n",
       " 'Bbf6',\n",
       " 'Bbf7',\n",
       " 'Bbg3',\n",
       " 'Bbg3+',\n",
       " 'Bbg6',\n",
       " 'Bbg6+',\n",
       " 'Bbg7',\n",
       " 'Bbh2',\n",
       " 'Bbxa3',\n",
       " 'Bbxc5',\n",
       " 'Bbxf4',\n",
       " 'Bbxf6',\n",
       " 'Bc1',\n",
       " 'Bc1#',\n",
       " 'Bc1+',\n",
       " 'Bc2',\n",
       " 'Bc2#',\n",
       " 'Bc2+',\n",
       " 'Bc3',\n",
       " 'Bc3#',\n",
       " 'Bc3+',\n",
       " 'Bc4',\n",
       " 'Bc4#',\n",
       " 'Bc4+',\n",
       " 'Bc4b3',\n",
       " 'Bc5',\n",
       " 'Bc5#',\n",
       " 'Bc5+',\n",
       " 'Bc6',\n",
       " 'Bc6#',\n",
       " 'Bc6+',\n",
       " 'Bc7',\n",
       " 'Bc7#',\n",
       " 'Bc7+',\n",
       " 'Bc8',\n",
       " 'Bc8#',\n",
       " 'Bc8+',\n",
       " 'Bca2',\n",
       " 'Bca3',\n",
       " 'Bca4',\n",
       " 'Bca5',\n",
       " 'Bca6',\n",
       " 'Bca7',\n",
       " 'Bcb1',\n",
       " 'Bcb2',\n",
       " 'Bcb2+',\n",
       " 'Bcb3',\n",
       " 'Bcb3+',\n",
       " 'Bcb4',\n",
       " 'Bcb5',\n",
       " 'Bcb5+',\n",
       " 'Bcb6',\n",
       " 'Bcb7',\n",
       " 'Bcb7+',\n",
       " 'Bcb8',\n",
       " 'Bcd1',\n",
       " 'Bcd2',\n",
       " 'Bcd2+',\n",
       " 'Bcd3',\n",
       " 'Bcd3+',\n",
       " 'Bcd4',\n",
       " 'Bcd4+',\n",
       " 'Bcd5',\n",
       " 'Bcd5+',\n",
       " 'Bcd6',\n",
       " 'Bcd6#',\n",
       " 'Bcd6+',\n",
       " 'Bcd7',\n",
       " 'Bcd7#',\n",
       " 'Bcd7+',\n",
       " 'Bcd8',\n",
       " 'Bce1',\n",
       " 'Bce2',\n",
       " 'Bce3',\n",
       " 'Bce3+',\n",
       " 'Bce4',\n",
       " 'Bce4+',\n",
       " 'Bce5',\n",
       " 'Bce5+',\n",
       " 'Bce6',\n",
       " 'Bce6#',\n",
       " 'Bce6+',\n",
       " 'Bce7',\n",
       " 'Bcf2',\n",
       " 'Bcf3+',\n",
       " 'Bcf4',\n",
       " 'Bcf4+',\n",
       " 'Bcf5',\n",
       " 'Bcf5+',\n",
       " 'Bcf6',\n",
       " 'Bcf6+',\n",
       " 'Bcf7',\n",
       " 'Bcf8',\n",
       " 'Bcg2',\n",
       " 'Bcg4',\n",
       " 'Bcg5',\n",
       " 'Bcg7',\n",
       " 'Bcg8',\n",
       " 'Bch2',\n",
       " 'Bch6',\n",
       " 'Bcxa3',\n",
       " 'Bcxb5',\n",
       " 'Bcxf4',\n",
       " 'Bcxg4',\n",
       " 'Bd1',\n",
       " 'Bd1#',\n",
       " 'Bd1+',\n",
       " 'Bd2',\n",
       " 'Bd2#',\n",
       " 'Bd2+',\n",
       " 'Bd3',\n",
       " 'Bd3#',\n",
       " 'Bd3+',\n",
       " 'Bd4',\n",
       " 'Bd4#',\n",
       " 'Bd4+',\n",
       " 'Bd5',\n",
       " 'Bd5#',\n",
       " 'Bd5+',\n",
       " 'Bd5e4',\n",
       " 'Bd6',\n",
       " 'Bd6#',\n",
       " 'Bd6+',\n",
       " 'Bd7',\n",
       " 'Bd7#',\n",
       " 'Bd7+',\n",
       " 'Bd8',\n",
       " 'Bd8#',\n",
       " 'Bd8+',\n",
       " 'Bda3',\n",
       " 'Bda4',\n",
       " 'Bdb1',\n",
       " 'Bdb2',\n",
       " 'Bdb3',\n",
       " 'Bdb4',\n",
       " 'Bdb5',\n",
       " 'Bdb5+',\n",
       " 'Bdb6',\n",
       " 'Bdb6#',\n",
       " 'Bdb6+',\n",
       " 'Bdb7',\n",
       " 'Bdb7+',\n",
       " 'Bdc1',\n",
       " 'Bdc2',\n",
       " 'Bdc2+',\n",
       " 'Bdc3',\n",
       " 'Bdc3+',\n",
       " 'Bdc4',\n",
       " 'Bdc4+',\n",
       " 'Bdc5',\n",
       " 'Bdc5#',\n",
       " 'Bdc5+',\n",
       " 'Bdc6',\n",
       " 'Bdc6+',\n",
       " 'Bdc7',\n",
       " 'Bdc7+',\n",
       " 'Bde1',\n",
       " 'Bde2',\n",
       " 'Bde2+',\n",
       " 'Bde3',\n",
       " 'Bde3+',\n",
       " 'Bde4',\n",
       " 'Bde4#',\n",
       " 'Bde4+',\n",
       " 'Bde5',\n",
       " 'Bde5#',\n",
       " 'Bde5+',\n",
       " 'Bde6',\n",
       " 'Bde6+',\n",
       " 'Bde7',\n",
       " 'Bde7+',\n",
       " 'Bdf2',\n",
       " 'Bdf2#',\n",
       " 'Bdf2+',\n",
       " 'Bdf3',\n",
       " 'Bdf3+',\n",
       " 'Bdf4',\n",
       " 'Bdf5',\n",
       " 'Bdf5+',\n",
       " 'Bdf6',\n",
       " 'Bdf6+',\n",
       " 'Bdf7',\n",
       " 'Bdf7+',\n",
       " 'Bdg3',\n",
       " 'Bdg4',\n",
       " 'Bdg5',\n",
       " 'Bdg6',\n",
       " 'Bdg6+',\n",
       " 'Bdg7',\n",
       " 'Bdh4',\n",
       " 'Bdh5',\n",
       " 'Bdh6',\n",
       " 'Bdh7',\n",
       " 'Bdxa3',\n",
       " 'Bdxc7',\n",
       " 'Bdxe6',\n",
       " 'Bdxf5',\n",
       " 'Bdxf6',\n",
       " 'Bdxf6#',\n",
       " 'Bdxf6+',\n",
       " 'Bdxh5',\n",
       " 'Bdxh6',\n",
       " 'Be1',\n",
       " 'Be1#',\n",
       " 'Be1+',\n",
       " 'Be2',\n",
       " 'Be2#',\n",
       " 'Be2+',\n",
       " 'Be3',\n",
       " 'Be3#',\n",
       " 'Be3+',\n",
       " 'Be4',\n",
       " 'Be4#',\n",
       " 'Be4+',\n",
       " 'Be5',\n",
       " 'Be5#',\n",
       " 'Be5+',\n",
       " 'Be5f4',\n",
       " 'Be5f6',\n",
       " 'Be6',\n",
       " 'Be6#',\n",
       " 'Be6+',\n",
       " 'Be7',\n",
       " 'Be7#',\n",
       " 'Be7+',\n",
       " 'Be8',\n",
       " 'Be8#',\n",
       " 'Be8+',\n",
       " 'Bea2',\n",
       " 'Bea3',\n",
       " 'Bea7+',\n",
       " 'Beb2',\n",
       " 'Beb4',\n",
       " 'Beb5',\n",
       " 'Beb5+',\n",
       " 'Beb7',\n",
       " 'Bec1',\n",
       " 'Bec2',\n",
       " 'Bec3',\n",
       " 'Bec3+',\n",
       " 'Bec4',\n",
       " 'Bec4+',\n",
       " 'Bec5',\n",
       " 'Bec6',\n",
       " 'Bec7',\n",
       " 'Bed2',\n",
       " 'Bed2#',\n",
       " 'Bed2+',\n",
       " 'Bed3',\n",
       " 'Bed3+',\n",
       " 'Bed4',\n",
       " 'Bed4+',\n",
       " 'Bed5',\n",
       " 'Bed5+',\n",
       " 'Bed6',\n",
       " 'Bed6+',\n",
       " 'Bed7',\n",
       " 'Bed7#',\n",
       " 'Bed7+',\n",
       " 'Bef1',\n",
       " 'Bef2',\n",
       " 'Bef2#',\n",
       " 'Bef2+',\n",
       " 'Bef3',\n",
       " 'Bef3+',\n",
       " 'Bef4',\n",
       " 'Bef4+',\n",
       " 'Bef5',\n",
       " 'Bef5+',\n",
       " 'Bef6',\n",
       " 'Bef6+',\n",
       " 'Bef7',\n",
       " 'Bef7+',\n",
       " 'Beg1',\n",
       " 'Beg2',\n",
       " 'Beg2#',\n",
       " 'Beg3',\n",
       " 'Beg5',\n",
       " 'Beg5+',\n",
       " 'Beg6',\n",
       " 'Beg6+',\n",
       " 'Beg7',\n",
       " 'Beh2',\n",
       " 'Beh4',\n",
       " 'Beh6',\n",
       " 'Beh7',\n",
       " 'Bexb7',\n",
       " 'Bexd2',\n",
       " 'Bexf2+',\n",
       " 'Bexf3',\n",
       " 'Bexf3#',\n",
       " 'Bexf4',\n",
       " 'Bexg2',\n",
       " 'Bexg7',\n",
       " 'Bf1',\n",
       " 'Bf1#',\n",
       " 'Bf1+',\n",
       " 'Bf2',\n",
       " 'Bf2#',\n",
       " 'Bf2+',\n",
       " 'Bf3',\n",
       " 'Bf3#',\n",
       " 'Bf3+',\n",
       " 'Bf4',\n",
       " 'Bf4#',\n",
       " 'Bf4+',\n",
       " 'Bf5',\n",
       " 'Bf5#',\n",
       " 'Bf5+',\n",
       " 'Bf6',\n",
       " 'Bf6#',\n",
       " 'Bf6+',\n",
       " 'Bf7',\n",
       " 'Bf7#',\n",
       " 'Bf7+',\n",
       " 'Bf8',\n",
       " 'Bf8#',\n",
       " 'Bf8+',\n",
       " 'Bf8g7',\n",
       " 'Bfa3',\n",
       " 'Bfa6',\n",
       " 'Bfb2',\n",
       " 'Bfb4',\n",
       " 'Bfb5',\n",
       " 'Bfb5#',\n",
       " 'Bfb7',\n",
       " 'Bfc1',\n",
       " 'Bfc2',\n",
       " 'Bfc2+',\n",
       " 'Bfc3',\n",
       " 'Bfc4',\n",
       " 'Bfc4#',\n",
       " 'Bfc5',\n",
       " 'Bfc6',\n",
       " 'Bfc7',\n",
       " 'Bfc8',\n",
       " 'Bfd1',\n",
       " 'Bfd2',\n",
       " 'Bfd3',\n",
       " 'Bfd3#',\n",
       " 'Bfd3+',\n",
       " 'Bfd4',\n",
       " 'Bfd4+',\n",
       " 'Bfd5',\n",
       " 'Bfd5#',\n",
       " 'Bfd5+',\n",
       " 'Bfd6',\n",
       " 'Bfd6+',\n",
       " 'Bfd7',\n",
       " 'Bfd7#',\n",
       " 'Bfd8',\n",
       " 'Bfe1',\n",
       " 'Bfe2',\n",
       " 'Bfe2#',\n",
       " 'Bfe2+',\n",
       " 'Bfe3',\n",
       " 'Bfe3#',\n",
       " 'Bfe3+',\n",
       " 'Bfe4',\n",
       " 'Bfe4#',\n",
       " 'Bfe4+',\n",
       " 'Bfe5',\n",
       " 'Bfe5#',\n",
       " 'Bfe5+',\n",
       " 'Bfe6',\n",
       " 'Bfe6+',\n",
       " 'Bfe7',\n",
       " 'Bfe7+',\n",
       " 'Bfe8',\n",
       " 'Bfg1',\n",
       " 'Bfg2',\n",
       " 'Bfg2#',\n",
       " 'Bfg3',\n",
       " 'Bfg4',\n",
       " 'Bfg5',\n",
       " 'Bfg6',\n",
       " 'Bfg6+',\n",
       " 'Bfg7',\n",
       " 'Bfg7#',\n",
       " 'Bfg7+',\n",
       " 'Bfg8',\n",
       " 'Bfh3',\n",
       " 'Bfh5',\n",
       " 'Bfh6',\n",
       " 'Bfh7',\n",
       " 'Bfxd5',\n",
       " 'Bfxe5',\n",
       " 'Bfxg2',\n",
       " 'Bfxg7',\n",
       " 'Bg1',\n",
       " 'Bg1#',\n",
       " 'Bg1+',\n",
       " 'Bg2',\n",
       " 'Bg2#',\n",
       " 'Bg2+',\n",
       " 'Bg3',\n",
       " 'Bg3#',\n",
       " 'Bg3+',\n",
       " 'Bg4',\n",
       " 'Bg4#',\n",
       " 'Bg4+',\n",
       " 'Bg5',\n",
       " 'Bg5#',\n",
       " 'Bg5+',\n",
       " 'Bg6',\n",
       " 'Bg6#',\n",
       " 'Bg6+',\n",
       " 'Bg7',\n",
       " 'Bg7#',\n",
       " 'Bg7+',\n",
       " 'Bg8',\n",
       " 'Bg8#',\n",
       " 'Bg8+',\n",
       " 'Bgb2',\n",
       " 'Bgb3',\n",
       " 'Bgb6',\n",
       " 'Bgb7',\n",
       " 'Bgc2+',\n",
       " 'Bgc3',\n",
       " 'Bgc4',\n",
       " 'Bgc4+',\n",
       " 'Bgc5',\n",
       " 'Bgc5+',\n",
       " 'Bgd1',\n",
       " 'Bgd2',\n",
       " 'Bgd3',\n",
       " 'Bgd3+',\n",
       " 'Bgd4',\n",
       " 'Bgd4+',\n",
       " 'Bgd5',\n",
       " 'Bgd5#',\n",
       " 'Bgd5+',\n",
       " 'Bge1',\n",
       " 'Bge2',\n",
       " 'Bge2#',\n",
       " 'Bge3',\n",
       " 'Bge3#',\n",
       " 'Bge3+',\n",
       " 'Bge4',\n",
       " 'Bge4+',\n",
       " 'Bge5',\n",
       " 'Bge6',\n",
       " 'Bge6#',\n",
       " 'Bge6+',\n",
       " 'Bge7',\n",
       " 'Bge8',\n",
       " 'Bgf1',\n",
       " 'Bgf2',\n",
       " 'Bgf2+',\n",
       " 'Bgf3',\n",
       " 'Bgf3+',\n",
       " 'Bgf4',\n",
       " 'Bgf4#',\n",
       " 'Bgf4+',\n",
       " 'Bgf5',\n",
       " 'Bgf5+',\n",
       " 'Bgf6',\n",
       " 'Bgf6+',\n",
       " 'Bgf7',\n",
       " 'Bgf7+',\n",
       " 'Bgf8',\n",
       " 'Bgh2',\n",
       " 'Bgh5',\n",
       " 'Bgh6',\n",
       " 'Bgh7',\n",
       " 'Bgxc5',\n",
       " 'Bgxe3',\n",
       " 'Bgxe3+',\n",
       " 'Bh1',\n",
       " 'Bh1#',\n",
       " 'Bh1+',\n",
       " 'Bh1f3',\n",
       " 'Bh2',\n",
       " 'Bh2#',\n",
       " 'Bh2+',\n",
       " 'Bh3',\n",
       " 'Bh3#',\n",
       " 'Bh3+',\n",
       " 'Bh4',\n",
       " 'Bh4#',\n",
       " 'Bh4+',\n",
       " 'Bh5',\n",
       " 'Bh5#',\n",
       " 'Bh5+',\n",
       " 'Bh6',\n",
       " 'Bh6#',\n",
       " 'Bh6+',\n",
       " 'Bh7',\n",
       " 'Bh7#',\n",
       " 'Bh7+',\n",
       " 'Bh8',\n",
       " 'Bh8#',\n",
       " 'Bh8+',\n",
       " 'Bhb1',\n",
       " 'Bhb2',\n",
       " 'Bhb7',\n",
       " 'Bhc2',\n",
       " 'Bhc3',\n",
       " 'Bhc3+',\n",
       " 'Bhc6',\n",
       " 'Bhc6+',\n",
       " 'Bhc7',\n",
       " 'Bhd2',\n",
       " 'Bhd3',\n",
       " 'Bhd3+',\n",
       " 'Bhd4',\n",
       " 'Bhd4#',\n",
       " 'Bhd4+',\n",
       " 'Bhd5',\n",
       " 'Bhd5#',\n",
       " 'Bhd5+',\n",
       " 'Bhd7',\n",
       " 'Bhe1',\n",
       " 'Bhe2',\n",
       " 'Bhe3',\n",
       " 'Bhe4',\n",
       " 'Bhe4+',\n",
       " 'Bhe5',\n",
       " 'Bhe5+',\n",
       " 'Bhe6',\n",
       " 'Bhe7',\n",
       " 'Bhe8',\n",
       " 'Bhf2',\n",
       " 'Bhf2+',\n",
       " 'Bhf3',\n",
       " 'Bhf3#',\n",
       " 'Bhf3+',\n",
       " 'Bhf4',\n",
       " 'Bhf5',\n",
       " 'Bhf5#',\n",
       " 'Bhf5+',\n",
       " 'Bhf6',\n",
       " 'Bhf6+',\n",
       " 'Bhf7',\n",
       " 'Bhg2',\n",
       " 'Bhg2#',\n",
       " 'Bhg2+',\n",
       " 'Bhg3',\n",
       " 'Bhg4',\n",
       " 'Bhg4#',\n",
       " 'Bhg4+',\n",
       " 'Bhg5',\n",
       " 'Bhg5+',\n",
       " 'Bhg6',\n",
       " 'Bhg7',\n",
       " 'Bhg7#',\n",
       " 'Bhg7+',\n",
       " 'Bhg8',\n",
       " 'Bhxc6',\n",
       " 'Bhxd3',\n",
       " 'Bhxg4',\n",
       " 'Bxa1',\n",
       " 'Bxa1#',\n",
       " 'Bxa1+',\n",
       " 'Bxa2',\n",
       " 'Bxa2#',\n",
       " 'Bxa2+',\n",
       " 'Bxa3',\n",
       " 'Bxa3#',\n",
       " 'Bxa3+',\n",
       " 'Bxa4',\n",
       " 'Bxa4#',\n",
       " 'Bxa4+',\n",
       " 'Bxa5',\n",
       " 'Bxa5#',\n",
       " 'Bxa5+',\n",
       " 'Bxa6',\n",
       " 'Bxa6#',\n",
       " 'Bxa6+',\n",
       " 'Bxa7',\n",
       " 'Bxa7#',\n",
       " 'Bxa7+',\n",
       " 'Bxa8',\n",
       " 'Bxa8#',\n",
       " 'Bxa8+',\n",
       " 'Bxb1',\n",
       " 'Bxb1#',\n",
       " 'Bxb1+',\n",
       " 'Bxb2',\n",
       " 'Bxb2#',\n",
       " 'Bxb2+',\n",
       " 'Bxb3',\n",
       " 'Bxb3#',\n",
       " 'Bxb3+',\n",
       " 'Bxb4',\n",
       " 'Bxb4#',\n",
       " 'Bxb4+',\n",
       " 'Bxb5',\n",
       " 'Bxb5#',\n",
       " 'Bxb5+',\n",
       " 'Bxb6',\n",
       " 'Bxb6#',\n",
       " 'Bxb6+',\n",
       " 'Bxb7',\n",
       " 'Bxb7#',\n",
       " 'Bxb7+',\n",
       " 'Bxb8',\n",
       " 'Bxb8#',\n",
       " 'Bxb8+',\n",
       " 'Bxc1',\n",
       " 'Bxc1#',\n",
       " 'Bxc1+',\n",
       " 'Bxc2',\n",
       " 'Bxc2#',\n",
       " 'Bxc2+',\n",
       " 'Bxc3',\n",
       " 'Bxc3#',\n",
       " 'Bxc3+',\n",
       " 'Bxc4',\n",
       " 'Bxc4#',\n",
       " 'Bxc4+',\n",
       " 'Bxc5',\n",
       " 'Bxc5#',\n",
       " 'Bxc5+',\n",
       " 'Bxc6',\n",
       " 'Bxc6#',\n",
       " 'Bxc6+',\n",
       " 'Bxc7',\n",
       " 'Bxc7#',\n",
       " 'Bxc7+',\n",
       " 'Bxc8',\n",
       " 'Bxc8#',\n",
       " 'Bxc8+',\n",
       " 'Bxd1',\n",
       " 'Bxd1#',\n",
       " 'Bxd1+',\n",
       " 'Bxd2',\n",
       " 'Bxd2#',\n",
       " 'Bxd2+',\n",
       " 'Bxd3',\n",
       " 'Bxd3#',\n",
       " 'Bxd3+',\n",
       " 'Bxd4',\n",
       " 'Bxd4#',\n",
       " 'Bxd4+',\n",
       " 'Bxd5',\n",
       " 'Bxd5#',\n",
       " 'Bxd5+',\n",
       " 'Bxd6',\n",
       " 'Bxd6#',\n",
       " 'Bxd6+',\n",
       " 'Bxd7',\n",
       " 'Bxd7#',\n",
       " 'Bxd7+',\n",
       " 'Bxd8',\n",
       " 'Bxd8#',\n",
       " 'Bxd8+',\n",
       " 'Bxe1',\n",
       " 'Bxe1#',\n",
       " 'Bxe1+',\n",
       " 'Bxe2',\n",
       " 'Bxe2#',\n",
       " 'Bxe2+',\n",
       " 'Bxe3',\n",
       " 'Bxe3#',\n",
       " 'Bxe3+',\n",
       " 'Bxe4',\n",
       " 'Bxe4#',\n",
       " 'Bxe4+',\n",
       " 'Bxe5',\n",
       " 'Bxe5#',\n",
       " 'Bxe5+',\n",
       " 'Bxe6',\n",
       " 'Bxe6#',\n",
       " 'Bxe6+',\n",
       " 'Bxe7',\n",
       " 'Bxe7#',\n",
       " 'Bxe7+',\n",
       " 'Bxe8',\n",
       " 'Bxe8#',\n",
       " 'Bxe8+',\n",
       " 'Bxf1',\n",
       " 'Bxf1#',\n",
       " 'Bxf1+',\n",
       " 'Bxf2',\n",
       " 'Bxf2#',\n",
       " 'Bxf2+',\n",
       " 'Bxf3',\n",
       " 'Bxf3#',\n",
       " 'Bxf3+',\n",
       " 'Bxf4',\n",
       " 'Bxf4#',\n",
       " 'Bxf4+',\n",
       " 'Bxf5',\n",
       " 'Bxf5#',\n",
       " 'Bxf5+',\n",
       " 'Bxf6',\n",
       " 'Bxf6#',\n",
       " 'Bxf6+',\n",
       " 'Bxf7',\n",
       " 'Bxf7#',\n",
       " 'Bxf7+',\n",
       " 'Bxf8',\n",
       " 'Bxf8#',\n",
       " 'Bxf8+',\n",
       " 'Bxg1',\n",
       " 'Bxg1#',\n",
       " 'Bxg1+',\n",
       " 'Bxg2',\n",
       " 'Bxg2#',\n",
       " 'Bxg2+',\n",
       " 'Bxg3',\n",
       " 'Bxg3#',\n",
       " 'Bxg3+',\n",
       " 'Bxg4',\n",
       " 'Bxg4#',\n",
       " 'Bxg4+',\n",
       " 'Bxg5',\n",
       " 'Bxg5#',\n",
       " 'Bxg5+',\n",
       " 'Bxg6',\n",
       " 'Bxg6#',\n",
       " 'Bxg6+',\n",
       " 'Bxg7',\n",
       " 'Bxg7#',\n",
       " 'Bxg7+',\n",
       " 'Bxg8',\n",
       " 'Bxg8#',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = open('vocab.txt', 'r').read().splitlines()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6173cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa369c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'vocab' is your list of moves as created in your code snippet\n",
    "move_to_index = {move: idx for idx, move in enumerate(vocab)}\n",
    "index_to_move = {idx: move for idx, move in enumerate(vocab)}\n",
    "\n",
    "# Add special tokens\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "move_to_index[UNK_TOKEN] = len(move_to_index)\n",
    "move_to_index[PAD_TOKEN] = len(move_to_index)\n",
    "index_to_move[len(index_to_move)] = UNK_TOKEN\n",
    "index_to_move[len(index_to_move)] = PAD_TOKEN\n",
    "\n",
    "def encode_move(move):\n",
    "    return move_to_index.get(move, move_to_index[UNK_TOKEN])\n",
    "\n",
    "def decode_move(index):\n",
    "    return index_to_move.get(index, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6e09f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8316"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_move('Qe4#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94a11b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qe4#'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_move(8316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed3b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (max_len, d_model) filled with zeros\n",
    "        # This will store the positional encodings for each position and dimension\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Create a vector of positions from 0 to max_len-1\n",
    "        # Unsqueeze to shape (max_len, 1) for broadcasting\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create the division term for the sinusoidal function\n",
    "        # This creates a vector of values that increase exponentially\n",
    "        # We use log(10000.0) as it's a common choice that works well in practice\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices in the positional encoding\n",
    "        # This creates a sinusoidal pattern that varies at different frequencies\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices in the positional encoding\n",
    "        # This creates a cosinusoidal pattern that varies at different frequencies\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Unsqueeze and transpose to shape (1, max_len, d_model)\n",
    "        # This allows for easy addition to the input embeddings later\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # Register the positional encoding as a buffer\n",
    "        # This means it won't be considered a model parameter (won't be updated during training)\n",
    "        # but will be saved and loaded with the model\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encoding to the input\n",
    "        # x is expected to have shape (seq_len, batch_size, d_model)\n",
    "        # We slice the positional encoding to match the input sequence length\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837caaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create an embedding layer to convert input tokens to vectors\n",
    "        # vocab_size is the number of unique tokens in our vocabulary\n",
    "        # d_model is the dimensionality of the embedding space\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Create a positional encoding layer\n",
    "        # This adds information about the position of each token in the sequence\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Create a single transformer encoder layer\n",
    "        # This includes self-attention and feedforward neural network\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        \n",
    "        # Create the full transformer encoder by stacking multiple encoder layers\n",
    "        # num_encoder_layers determines the depth of the network\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # Store d_model for use in the forward pass\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create a linear layer for the final output\n",
    "        # This projects the transformer output back to vocabulary space\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Convert input tokens to embeddings\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings\n",
    "        # This scaling helps maintain the variance of the forward pass\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding to the embeddings\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Pass the encoded input through the transformer encoder\n",
    "        output = self.transformer_encoder(src)\n",
    "        \n",
    "        # Project the transformer output to vocabulary space\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c38e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_length=100):\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_move = self.data[idx]\n",
    "        \n",
    "        # Ensure input_seq is a list\n",
    "        input_seq = input_seq.split() if isinstance(input_seq, str) else input_seq\n",
    "        \n",
    "        # Truncate or pad input sequence to exactly max_seq_length\n",
    "        if len(input_seq) > self.max_seq_length:\n",
    "            input_seq = input_seq[-self.max_seq_length:]\n",
    "        else:\n",
    "            input_seq = [PAD_TOKEN] * (self.max_seq_length - len(input_seq)) + input_seq\n",
    "        \n",
    "        # Ensure the sequence is exactly max_seq_length\n",
    "        input_seq = input_seq[:self.max_seq_length]\n",
    "        \n",
    "        # Encode moves\n",
    "        input_tensor = torch.tensor([encode_move(m) for m in input_seq])\n",
    "        target_tensor = torch.tensor(encode_move(target_move))\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826d60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = ChessDataset(training_data, max_seq_length=100)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7047d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "vocab_size = len(vocab) + 500  # Slightly larger than the actual vocabulary size                                                    (1/5 results) [1919/2674]\n",
    "d_model = 512                                                                                                                                     \n",
    "nhead = 8\n",
    "num_encoder_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26dc287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "# Model definition\n",
    "model = ChessTransformer(vocab_size, d_model, nhead, num_encoder_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Clear cache (might not need this)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9cdca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device, log_interval=100, save_interval=1000):\n",
    "    model.to(device)\n",
    "    total_steps = len(dataloader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(dataloader), total=total_steps, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for i, (inputs, targets) in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs[:, -1, :], targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % log_interval == 0:\n",
    "                avg_loss = running_loss / log_interval\n",
    "                progress_bar.set_postfix({'Loss': f'{avg_loss:.4f}'})\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            if (i + 1) % save_interval == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, f'checkpoint_epoch{epoch+1}_step{i+1}.pth')\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_avg_loss = epoch_loss / total_steps\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s - Avg Loss: {epoch_avg_loss:.4f}\")\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9}GB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_cached()/1e9}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                | 115/237464 [01:38<508:19:02,  7.71s/it, Loss=6.8377]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "log_interval = 100  # Log every 100 batches\n",
    "save_interval = 1000  # Save checkpoint every 1000 batches\n",
    "\n",
    "print(\"Starting training loop...\")\n",
    "total_start_time =- time.perf_counter()\n",
    "\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs, device, log_interval, save_interval)\n",
    "\n",
    "total_end_time = time.perf_counter()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"Total training time: {total_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a66ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e93be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_next_move(model, move_sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([encode_move(m) for m in move_sequence.split()]).unsqueeze(0).to(device)\n",
    "        output = model(input_tensor)\n",
    "        predicted_move_index = output[0, -1, :].argmax().item()\n",
    "        return decode_move(predicted_move_index)\n",
    "\n",
    "# Example usage\n",
    "game_so_far = \"e4\"\n",
    "next_move = predict_next_move(model, game_so_far)\n",
    "print(f\"Predicted next move: {next_move}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
